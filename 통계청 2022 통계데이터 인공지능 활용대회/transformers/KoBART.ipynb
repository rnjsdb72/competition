{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"HLpof1pofV90"},"outputs":[],"source":["!pip install transformers\n","!pip install sentencepiece\n","!pip install git+https://github.com/SKT-AI/KoBART#egg=kobart"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2434,"status":"ok","timestamp":1648520069796,"user":{"displayName":"권유진","userId":"17694849594474805342"},"user_tz":-540},"id":"1za05WcKfBoZ"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from glob import glob\n","import missingno as msno\n","from tqdm import tqdm, trange\n","import pickle\n","import random\n","\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.metrics import make_scorer, accuracy_score, f1_score\n","from keras.preprocessing.sequence import pad_sequences\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from transformers import BartTokenizer, BartModel, BartForSequenceClassification\n","from kobart import get_pytorch_kobart_model, get_kobart_tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4758,"status":"ok","timestamp":1648520074550,"user":{"displayName":"권유진","userId":"17694849594474805342"},"user_tz":-540},"id":"lPMsX6pRfRYG"},"outputs":[],"source":["files = glob('/content/drive/MyDrive/공모전/data/*.txt')\n","for i, file in enumerate(files):\n","    globals()[f'file{i}'] = pd.read_table(file, sep='|', encoding='cp949')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1648520074552,"user":{"displayName":"권유진","userId":"17694849594474805342"},"user_tz":-540},"id":"fimro-enfaHw"},"outputs":[],"source":["idx2label_digit1 = dict(enumerate(sorted(file0.digit_1.unique())))\n","label2idx_digit1 = {label:idx for idx, label in enumerate(sorted(file0.digit_1.unique()))}\n","idx2label_digit2 = dict(enumerate(sorted(file0.digit_2.unique())))\n","label2idx_digit2 = {label:idx for idx, label in enumerate(sorted(file0.digit_2.unique()))}\n","idx2label_digit3 = dict(enumerate(sorted(file0.digit_3.unique())))\n","label2idx_digit3 = {label:idx for idx, label in enumerate(sorted(file0.digit_3.unique()))}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J49ge8LDfbUQ"},"outputs":[],"source":["file0['digit_1'] = file0['digit_1'].map(lambda x: label2idx_digit1[x])\n","file0['digit_2'] = file0['digit_2'].map(lambda x: label2idx_digit2[x])\n","file0['digit_3'] = file0['digit_3'].map(lambda x: label2idx_digit3[x])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ok-yJpidi7Yz"},"outputs":[],"source":["sentences_raw = file0[['text_obj','text_mthd', 'text_deal']].fillna('').apply(lambda x: ' '.join(x).strip(), axis=1)\n","labels_raw = file0[['digit_1','digit_2','digit_3']].values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h6iG4CeBTVi8"},"outputs":[],"source":["sentences, sentences_test, labels, labels_test = train_test_split(sentences_raw, labels_raw, test_size=0.3, random_state=0)"]},{"cell_type":"markdown","metadata":{"id":"7b-8ekIuME1M"},"source":["### KoBART"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CyQSegjYS-ho"},"outputs":[],"source":["if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1648485240585,"user":{"displayName":"권유진","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17694849594474805342"},"user_tz":-540},"id":"cENhH3kgK3ZJ","outputId":"30add423-0bf9-47bf-afdd-02a94f83e92d"},"outputs":[],"source":["print(torch.cuda.get_device_name(0))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":66652,"status":"ok","timestamp":1648485307232,"user":{"displayName":"권유진","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17694849594474805342"},"user_tz":-540},"id":"wxHXQdEJT0sL","outputId":"4b1ca36c-1249-4ada-8ce0-c935c87da880"},"outputs":[],"source":["tokenizer = get_kobart_tokenizer()\n","tokenized_texts = list(map(lambda x: tokenizer.tokenize(x, return_tensors='pt'), sentences))\n","print (\"Tokenize the first sentence:\")\n","print (tokenized_texts[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dxpo0N7YBvB"},"outputs":[],"source":["def tokenize_inputs(text_list, tokenizer, num_embeddings=120):\n","    \"\"\"\n","    Tokenizes the input text input into ids. Appends the appropriate special\n","    characters to the end of the text to denote end of sentence. Truncate or pad\n","    the appropriate sequence length.\n","    \"\"\"\n","    tokenized_texts = list(map(lambda x: tokenizer.tokenize(x, return_tensors='pt'), text_list))\n","    # convert tokenized text into numeric ids for the appropriate LM\n","    input_ids = list(map(lambda x: tokenizer.convert_tokens_to_ids(x), tokenized_texts))\n","    # pad sequences\n","    input_ids = pad_sequences(input_ids, maxlen=num_embeddings, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","    return input_ids\n","\n","def create_attn_masks(input_ids):\n","    \"\"\"\n","    Create attention masks to tell model whether attention should be applied to\n","    the input id tokens. Do not want to perform attention on padding tokens.\n","    \"\"\"\n","    # Create attention masks\n","    attention_masks = []\n","\n","    # Create a mask of 1s for each token followed by 0s for padding\n","    for seq in input_ids:\n","        seq_mask = [float(i>0) for i in seq]\n","        attention_masks.append(seq_mask)\n","    return attention_masks"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":140989,"status":"ok","timestamp":1648472776609,"user":{"displayName":"권유진","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17694849594474805342"},"user_tz":-540},"id":"JaIT0G2EYOhM","outputId":"20ea3f56-146c-49d2-8991-a31109fee791"},"outputs":[],"source":["input_ids = tokenize_inputs(sentences, tokenizer, num_embeddings=120)\n","attention_masks = create_attn_masks(input_ids)\n","input_ids = torch.from_numpy(input_ids)\n","attention_masks = torch.tensor(attention_masks)\n","\n","labels1 = torch.tensor(labels[:,0])\n","labels2 = torch.tensor(labels[:,1])\n","labels3 = torch.tensor(labels[:,2])\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', sentences[1])\n","print('Token IDs:', input_ids[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xM_2g587YzI1"},"outputs":[],"source":["from torch.utils.data import TensorDataset\n","\n","# Combine the training inputs into a TensorDataset.\n","dataset1 = TensorDataset(input_ids, attention_masks, labels1)\n","dataset2 = TensorDataset(input_ids, attention_masks, labels2)\n","dataset3 = TensorDataset(input_ids, attention_masks, labels3)\n","\n","# Divide the dataset by randomly selecting samples.\n","train_dataset1, val_dataset1 = train_test_split(dataset1, test_size=0.3, random_state=0)\n","train_dataset2, val_dataset2 = train_test_split(dataset2, test_size=0.3, random_state=0)\n","train_dataset3, val_dataset3 = train_test_split(dataset3, test_size=0.3, random_state=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0y919MW8YzI1"},"outputs":[],"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# The DataLoader needs to know our batch size for training, so we specify it \n","# here. Batch size of 16 or 32.\n","batch_size = 32\n","\n","# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","train_dataloader1 = DataLoader(\n","            train_dataset1,  # The training samples.\n","            sampler = RandomSampler(train_dataset1), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","train_dataloader2 = DataLoader(\n","            train_dataset2,  # The training samples.\n","            sampler = RandomSampler(train_dataset2), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","train_dataloader3 = DataLoader(\n","            train_dataset3,  # The training samples.\n","            sampler = RandomSampler(train_dataset3), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","validation_dataloader1 = DataLoader(\n","            val_dataset1, # The validation samples.\n","            sampler = SequentialSampler(val_dataset1), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )\n","validation_dataloader2 = DataLoader(\n","            val_dataset2, # The validation samples.\n","            sampler = SequentialSampler(val_dataset2), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )\n","validation_dataloader3 = DataLoader(\n","            val_dataset3, # The validation samples.\n","            sampler = SequentialSampler(val_dataset3), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ENtODxAPfrP1"},"outputs":[],"source":["class KoBARTClassification(torch.nn.Module):\n","    def __init__(self, num_labels):\n","        super(KoBARTClassification, self).__init__()\n","        self.num_labels = num_labels\n","        self.kobart = BartModel.from_pretrained(get_pytorch_kobart_model())\n","        self.classifier = torch.nn.Linear(768, num_labels)\n","\n","        torch.nn.init.xavier_normal_(self.classifier.weight)\n","    \n","    def forward(self, input_ids, attention_mask):\n","        last_hidden_state = self.kobart(input_ids=input_ids,\\\n","                                   attention_mask=attention_mask)\n","        mean_last_hidden_state = self.pool_hidden_state(last_hidden_state)\n","        logits = self.classifier(mean_last_hidden_state)\n","        return logits\n","\n","    def pool_hidden_state(self, last_hidden_state):\n","        \"\"\"\n","        Pool the output vectors into a single mean vector \n","        \"\"\"\n","        last_hidden_state = last_hidden_state[0]\n","        mean_last_hidden_state = torch.mean(last_hidden_state, 1)\n","        return mean_last_hidden_state"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NEszLLbWrABL"},"outputs":[],"source":["def train(model, num_epochs,\\\n","          optimizer, criterion,\\\n","          train_dataloader, valid_dataloader,\\\n","          model_save_path,\\\n","          train_loss_set=[], valid_loss_set = [],\\\n","          lowest_eval_loss=None, start_epoch=0,\\\n","          device=\"cpu\"\n","          ):\n","  \"\"\"\n","  Train the model and save the model with the lowest validation loss\n","  \"\"\"\n","  # We'll store a number of quantities such as training and validation loss, \n","  # validation accuracy, and timings.\n","  training_stats = []\n","  # Measure the total training time for the whole run.\n","  total_t0 = time.time()\n","\n","  model.to(device)\n","\n","  # trange is a tqdm wrapper around the normal python range\n","  for i in trange(num_epochs, desc=\"Epoch\"):\n","    # if continue training from saved model\n","    actual_epoch = start_epoch + i\n","\n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set. \n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(actual_epoch, num_epochs))\n","    print('Training...')\n","    \n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","    \n","    # Set our model to training mode (as opposed to evaluation mode)\n","    model.train()\n","\n","    # Tracking variables\n","    tr_loss = 0\n","    num_train_samples = 0\n","\n","    # Train the data for one epoch\n","    for step, batch in enumerate(train_dataloader):\n","        # Progress update every 100 batches.\n","        if step % 100 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.    Train Loss: {:}.    Train Accuracy: {:}.'.format(step, len(train_dataloader), elapsed, loss.item(), avg_train_accuracy))\n","            \n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask, b_labels = batch\n","        # Clear out the gradients (by default they accumulate)\n","        optimizer.zero_grad()\n","        # Forward pass\n","        logits = model(b_input_ids, b_input_mask)\n","        loss = criterion(logits, b_labels)\n","        # store train loss\n","        tr_loss += loss.item()\n","        num_train_samples += b_labels.size(0)\n","        # Backward pass\n","        loss.backward()\n","        # Update parameters and take a step using the computed gradient\n","        optimizer.step()\n","        #scheduler.step()\n","        # Accuracy\n","        prediction = logits.max(1, keepdim=True)[1]\n","        total_train_accuracy = prediction.eq(b_labels.view_as(prediction)).sum().item()\n","        avg_train_accuracy = total_train_accuracy / len(b_input_ids)\n","\n","    # Update tracking variables\n","    epoch_train_loss = tr_loss/num_train_samples\n","    train_loss_set.append(epoch_train_loss)\n","\n","#     print(\"Train loss: {}\".format(epoch_train_loss))\n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(epoch_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","    \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","    \n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","    \n","    # Put model in evaluation mode to evaluate loss on the validation set\n","    model.eval()\n","\n","    # Tracking variables \n","    eval_loss = 0\n","    num_eval_samples = 0\n","    total_eval_accuracy = 0 \n","\n","    # Evaluate data for one epoch\n","    for batch in valid_dataloader:\n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask, b_labels = batch\n","        # Telling the model not to compute or store gradients,\n","        # saving memory and speeding up validation\n","        with torch.no_grad():\n","            # Forward pass, calculate validation loss\n","            logits = model(b_input_ids, b_input_mask)\n","            loss = criterion(logits, b_labels)\n","            # store valid loss\n","            eval_loss += loss.item()\n","            num_eval_samples += b_labels.size(0)\n","            prediction = logits.max(1, keepdim=True)[1]\n","            total_eval_accuracy += prediction.eq(b_labels.view_as(prediction)).sum().item()\n","\n","    epoch_eval_loss = eval_loss/num_eval_samples\n","    valid_loss_set.append(epoch_eval_loss)\n","\n","#     print(\"Valid loss: {}\".format(epoch_eval_loss))\n","    \n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / num_eval_samples\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","    # Calculate the average loss over all of the batches.\n","#     avg_val_loss = total_eval_loss / num_eval_samples\n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","    \n","    print(\"  Validation Loss: {0:.2f}\".format(epoch_eval_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","    # Record all statistics from this epoch.\n","    training_stats.append(\n","        {\n","            'epoch': actual_epoch,\n","            'Training Loss': epoch_train_loss,\n","            'Valid. Loss': epoch_eval_loss,\n","             'Valid. Accur.': avg_val_accuracy,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time\n","        }\n","    )\n","\n","    \n","    if lowest_eval_loss == None:\n","      lowest_eval_loss = epoch_eval_loss\n","      # save model\n","      save_model(model, model_save_path, actual_epoch,\\\n","                 lowest_eval_loss, train_loss_set, valid_loss_set)\n","    else:\n","      if epoch_eval_loss < lowest_eval_loss:\n","        lowest_eval_loss = epoch_eval_loss\n","        # save model\n","        save_model(model, model_save_path, actual_epoch,\\\n","                   lowest_eval_loss, train_loss_set, valid_loss_set)\n","  \n","  print(\"\")\n","  print(\"Training complete!\")\n","\n","  print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n","  return model, train_loss_set, valid_loss_set, training_stats"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qjr0awjorABL"},"outputs":[],"source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ojHKhnBqrABM"},"outputs":[],"source":["# function to save and load the model form a specific epoch\n","def save_model(model, save_path, epochs, lowest_eval_loss, train_loss_hist, valid_loss_hist):\n","  \"\"\"\n","  Save the model to the path directory provided\n","  \"\"\"\n","  model_to_save = model.module if hasattr(model, 'module') else model\n","  checkpoint = {'epochs': epochs, \\\n","                'lowest_eval_loss': lowest_eval_loss,\\\n","                'state_dict': model_to_save.state_dict(),\\\n","                'train_loss_hist': train_loss_hist,\\\n","                'valid_loss_hist': valid_loss_hist\n","               }\n","  torch.save(checkpoint, save_path)\n","  print(\"Saving model at epoch {} with validation loss of {}\".format(epochs,\\\n","                                                                     lowest_eval_loss))\n","  return\n","  \n","def load_model(save_path):\n","  \"\"\"\n","  Load the model from the path directory provided\n","  \"\"\"\n","  checkpoint = torch.load(save_path)\n","  model_state_dict = checkpoint['state_dict']\n","  model = KoBARTClassification(num_labels=model_state_dict[\"classifier.weight\"].size()[0])\n","  model.load_state_dict(model_state_dict)\n","\n","  epochs = checkpoint[\"epochs\"]\n","  lowest_eval_loss = checkpoint[\"lowest_eval_loss\"]\n","  train_loss_hist = checkpoint[\"train_loss_hist\"]\n","  valid_loss_hist = checkpoint[\"valid_loss_hist\"]\n","  \n","  return model, epochs, lowest_eval_loss, train_loss_hist, valid_loss_hist"]},{"cell_type":"markdown","metadata":{"id":"RPgSe2qildW-"},"source":["#### Digit_1\n","- 2 epoch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W_3xbqVhrABM"},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5438,"status":"ok","timestamp":1648308760160,"user":{"displayName":"권유진","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17694849594474805342"},"user_tz":-540},"id":"p8Ras_BbteUT","outputId":"d1451d7e-0946-4c6a-b919-ade674852e12"},"outputs":[],"source":["num_epochs = 3\n","\n","model1 = KoBARTClassification(num_labels=len(labels1.unique())).to(device)\n","optimizer1 = torch.optim.AdamW(model1.parameters(),\n","                  lr = 5e-5, # args.learning_rate - default is 5e-5\n","                  # eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                 weight_decay=0.01,\n","                )\n","criterion1 = torch.nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23879860,"status":"ok","timestamp":1648332640014,"user":{"displayName":"권유진","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17694849594474805342"},"user_tz":-540},"id":"QdjKlJF6rABM","outputId":"2e24d46f-ae41-40ee-c498-6af120fd38d6"},"outputs":[],"source":["torch.manual_seed(0)\n","torch.cuda.manual_seed(0)\n","torch.cuda.manual_seed_all(0)\n","np.random.seed(0)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","random.seed(0)\n","\n","model_save_path = output_model_file = '/content/drive/MyDrive/공모전/models/kobart1.pt'\n","\n","model1, train_loss_set1, valid_loss_set1, training_stats1 = train(model=model1,\\\n","                                                              num_epochs=num_epochs,\\\n","                                                              optimizer=optimizer1,\\\n","                                                              criterion=criterion1,\\\n","                                                              train_dataloader=train_dataloader1,\\\n","                                                              valid_dataloader=validation_dataloader1,\\\n","                                                              model_save_path=model_save_path,\\\n","                                                              device=\"cuda\"\n","                                                              )"]},{"cell_type":"markdown","metadata":{"id":"7Ne94xVzlfPL"},"source":["#### Digit_2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eEr7bDPxlsnS"},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4775,"status":"ok","timestamp":1648332644779,"user":{"displayName":"권유진","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17694849594474805342"},"user_tz":-540},"id":"2GZJTd-AlsnS","outputId":"66058270-8d91-4041-a2d2-9a974fc2fab3"},"outputs":[],"source":["num_epochs = 3\n","\n","model2 = KoBARTClassification(num_labels=len(labels2.unique())).to(device)\n","optimizer2 = torch.optim.AdamW(model2.parameters(),\n","                  lr = 5e-5, # args.learning_rate - default is 5e-5\n","                  # eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                 weight_decay=0.01,\n","                )\n","criterion2 = torch.nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23890507,"status":"ok","timestamp":1648356535274,"user":{"displayName":"권유진","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17694849594474805342"},"user_tz":-540},"id":"zRSZXydklsnT","outputId":"bf4ad3c2-b719-4f7a-f3eb-8d61823cdd6f"},"outputs":[],"source":["torch.manual_seed(0)\n","torch.cuda.manual_seed(0)\n","torch.cuda.manual_seed_all(0)\n","np.random.seed(0)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","random.seed(0)\n","\n","model_save_path = output_model_file = '/content/drive/MyDrive/공모전/models/kobart2.pt'\n","\n","model2, train_loss_set2, valid_loss_set2, training_stats2 = train(model=model2,\\\n","                                                              num_epochs=num_epochs,\\\n","                                                              optimizer=optimizer2,\\\n","                                                              criterion=criterion2,\\\n","                                                              train_dataloader=train_dataloader2,\\\n","                                                              valid_dataloader=validation_dataloader2,\\\n","                                                              model_save_path=model_save_path,\\\n","                                                              device=\"cuda\"\n","                                                              )"]},{"cell_type":"markdown","metadata":{"id":"yBL3pmVFwZ6r"},"source":["```python\n","train_loss_set1 = [0.0046230769887563, 0.002806043499286586, 0.0022028916692519228]\n","train_loss_set2 = [0.009797783909549898, 0.006345497326163708, 0.005217890849337457]\n","valid_loss_set2 = [0.007217050700880853, 0.007007619538549555, 0.006949771315930412]\n"," ```"]},{"cell_type":"markdown","metadata":{"id":"bQdIM1ZjlhQb"},"source":["#### Digit_3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rnWgBNPpluYy"},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5565,"status":"ok","timestamp":1648398589646,"user":{"displayName":"권유진","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17694849594474805342"},"user_tz":-540},"id":"AV1UF4XzluYy","outputId":"3e7a6e0a-58aa-4a73-e9c6-23cab09f698d"},"outputs":[],"source":["num_epochs = 5\n","\n","model3 = KoBARTClassification(num_labels=len(labels3.unique())).to(device)\n","optimizer3 = torch.optim.AdamW(model3.parameters(),\n","                  lr = 5e-5, # args.learning_rate - default is 5e-5\n","                  # eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                 weight_decay=0.01,\n","                )\n","criterion3 = torch.nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TsEMTq3sluYy"},"outputs":[],"source":["torch.manual_seed(0)\n","torch.cuda.manual_seed(0)\n","torch.cuda.manual_seed_all(0)\n","np.random.seed(0)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","random.seed(0)\n","model_save_path = output_model_file = '/content/drive/MyDrive/공모전/models/kobart3.pt'\n","\n","model3, train_loss_set3, valid_loss_set3, training_stats3 = train(model=model3,\\\n","                                                              num_epochs=num_epochs,\\\n","                                                              optimizer=optimizer3,\\\n","                                                              criterion=criterion3,\\\n","                                                              train_dataloader=train_dataloader3,\\\n","                                                              valid_dataloader=validation_dataloader3,\\\n","                                                              model_save_path=model_save_path,\\\n","                                                              device=\"cuda\"\n","                                                              )"]},{"cell_type":"markdown","metadata":{"id":"4VBLj7Zc11Ng"},"source":["```python\n","torch.manual_seed(0)\n","torch.cuda.manual_seed(0)\n","torch.cuda.manual_seed_all(0)\n","np.random.seed(0)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","random.seed(0)\n","```"]},{"cell_type":"markdown","metadata":{"id":"EZvMbxrAkvEu"},"source":["#### Test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44283,"status":"ok","timestamp":1648485439719,"user":{"displayName":"권유진","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17694849594474805342"},"user_tz":-540},"id":"nK5CTb7W178v","outputId":"4f4c101f-4194-42a7-9201-7a80f2dec743"},"outputs":[],"source":["model1, epochs1, lowest_eval_loss1, train_loss_hist1, valid_loss_hist1 = load_model('/content/drive/MyDrive/공모전/models/kobart1.pt')\n","model2, epochs2, lowest_eval_loss2, train_loss_hist2, valid_loss_hist2 = load_model('/content/drive/MyDrive/공모전/models/kobart2.pt')\n","model3, epochs3, lowest_eval_loss3, train_loss_hist3, valid_loss_hist3 = load_model('/content/drive/MyDrive/공모전/models/kobart3.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4HG4pSOokuz1"},"outputs":[],"source":["input_ids_test = tokenize_inputs(sentences_test, tokenizer, num_embeddings=120)\n","attention_masks_test = create_attn_masks(input_ids_test)\n","input_ids_test = torch.from_numpy(input_ids_test)\n","attention_masks_test = torch.tensor(attention_masks_test)\n","\n","labels1_test = torch.tensor(labels_test[:,0])\n","labels2_test = torch.tensor(labels_test[:,1])\n","labels3_test = torch.tensor(labels_test[:,2])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kw1iZfcRYMS3"},"outputs":[],"source":["from torch.utils.data import TensorDataset\n","\n","# Combine the training inputs into a TensorDataset.\n","test_dataset1 = TensorDataset(input_ids_test, attention_masks_test, labels1_test)\n","test_dataset2 = TensorDataset(input_ids_test, attention_masks_test, labels2_test)\n","test_dataset3 = TensorDataset(input_ids_test, attention_masks_test, labels3_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xzVeqZFzYbYV"},"outputs":[],"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# The DataLoader needs to know our batch size for training, so we specify it \n","# here. Batch size of 16 or 32.\n","batch_size = 32\n","\n","test_dataloader1 = DataLoader(\n","            test_dataset1, # The validation samples.\n","            sampler = SequentialSampler(test_dataset1), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )\n","test_dataloader2 = DataLoader(\n","            test_dataset2, # The validation samples.\n","            sampler = SequentialSampler(test_dataset2), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )\n","test_dataloader3 = DataLoader(\n","            test_dataset3, # The validation samples.\n","            sampler = SequentialSampler(test_dataset3), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2703182,"status":"ok","timestamp":1648488449270,"user":{"displayName":"권유진","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17694849594474805342"},"user_tz":-540},"id":"OUZsUVV9Yzpa","outputId":"87970ad0-3357-4a27-ba4b-c9aa88b6a022"},"outputs":[],"source":["model = model1.to(device)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","eval_loss = 0\n","num_eval_samples = 0\n","total_eval_accuracy=0\n","for batch in test_dataloader1:\n","    # Add batch to GPU\n","    batch = tuple(t.to(device) for t in batch)\n","    # Unpack the inputs from our dataloader\n","    b_input_ids, b_input_mask, b_labels = batch\n","    # Telling the model not to compute or store gradients,\n","    # saving memory and speeding up validation\n","    with torch.no_grad():\n","        # Forward pass, calculate validation loss\n","        logits = model(b_input_ids, b_input_mask)\n","        loss = criterion(logits, b_labels)\n","        # store valid loss\n","        eval_loss += loss.item()\n","        num_eval_samples += b_labels.size(0)\n","        prediction1 = logits.max(1, keepdim=True)[1]\n","        total_eval_accuracy += prediction1.eq(b_labels.view_as(prediction1)).sum().item()\n","epoch_eval_loss = eval_loss/num_eval_samples\n","avg_val_accuracy = total_eval_accuracy / num_eval_samples\n","print(f'Loss: {epoch_eval_loss}\\t Accuracy: {avg_val_accuracy}')\n","# Loss: 0.0030046332769961253\t Accuracy: 0.9745533333333334"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2716607,"status":"ok","timestamp":1648491167193,"user":{"displayName":"권유진","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17694849594474805342"},"user_tz":-540},"id":"ZLFYm6sHZyR_","outputId":"5a574acc-50cf-4aa6-d275-fab2f2ae5eb3"},"outputs":[],"source":["model = model2.to(device)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","eval_loss = 0\n","num_eval_samples = 0\n","total_eval_accuracy=0\n","for batch in test_dataloader2:\n","    # Add batch to GPU\n","    batch = tuple(t.to(device) for t in batch)\n","    # Unpack the inputs from our dataloader\n","    b_input_ids, b_input_mask, b_labels = batch\n","    # Telling the model not to compute or store gradients,\n","    # saving memory and speeding up validation\n","    with torch.no_grad():\n","        # Forward pass, calculate validation loss\n","        logits = model(b_input_ids, b_input_mask)\n","        loss = criterion(logits, b_labels)\n","        # store valid loss\n","        eval_loss += loss.item()\n","        num_eval_samples += b_labels.size(0)\n","        prediction2 = logits.max(1, keepdim=True)[1]\n","        total_eval_accuracy += prediction2.eq(b_labels.view_as(prediction2)).sum().item()\n","epoch_eval_loss = eval_loss/num_eval_samples\n","avg_val_accuracy = total_eval_accuracy / num_eval_samples\n","print(f'Loss: {epoch_eval_loss}\\t Accuracy: {avg_val_accuracy}')\n","# Loss: 0.00708571706997153\t Accuracy: 0.9391"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2730601,"status":"ok","timestamp":1648493897789,"user":{"displayName":"권유진","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17694849594474805342"},"user_tz":-540},"id":"Wh9MtUjWaL2v","outputId":"413d36a9-b479-4d8b-80b5-decf56d8f895"},"outputs":[],"source":["model = model3.to(device)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","eval_loss = 0\n","num_eval_samples = 0\n","total_eval_accuracy=0\n","for batch in test_dataloader3:\n","    # Add batch to GPU\n","    batch = tuple(t.to(device) for t in batch)\n","    # Unpack the inputs from our dataloader\n","    b_input_ids, b_input_mask, b_labels = batch\n","    # Telling the model not to compute or store gradients,\n","    # saving memory and speeding up validation\n","    with torch.no_grad():\n","        # Forward pass, calculate validation loss\n","        logits = model(b_input_ids, b_input_mask)\n","        loss = criterion(logits, b_labels)\n","        # store valid loss\n","        eval_loss += loss.item()\n","        num_eval_samples += b_labels.size(0)\n","        prediction3 = logits.max(1, keepdim=True)[1]\n","        total_eval_accuracy += prediction3.eq(b_labels.view_as(prediction3)).sum().item()\n","epoch_eval_loss = eval_loss/num_eval_samples\n","avg_val_accuracy = total_eval_accuracy / num_eval_samples\n","print(f'Loss: {epoch_eval_loss}\\t Accuracy: {avg_val_accuracy}')\n","# Loss: 0.010448561390587129\t Accuracy: 0.9145666666666666"]},{"cell_type":"markdown","metadata":{"id":"EwExCD_GdYfG"},"source":["#### Submission"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FwDD42-pdXWv"},"outputs":[],"source":["sentences_sub = file1[['text_obj','text_mthd', 'text_deal']].fillna('').apply(lambda x: ' '.join(x).strip(), axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XaiapUKRdinI"},"outputs":[],"source":["input_ids_sub = tokenize_inputs(sentences_sub, tokenizer, num_embeddings=120)\n","attention_masks_sub = create_attn_masks(input_ids_sub)\n","input_ids_sub = torch.from_numpy(input_ids_sub)\n","attention_masks_sub = torch.tensor(attention_masks_sub)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4NWRw0MKdvQM"},"outputs":[],"source":["from torch.utils.data import TensorDataset\n","\n","# Combine the training inputs into a TensorDataset.\n","dataset_sub = TensorDataset(input_ids_sub, attention_masks_sub)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1BwPJqTid2is"},"outputs":[],"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# The DataLoader needs to know our batch size for training, so we specify it \n","# here. Batch size of 16 or 32.\n","batch_size = 32\n","\n","sub_dataloader = DataLoader(\n","            dataset_sub, # The validation samples.\n","            sampler = SequentialSampler(dataset_sub), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JQ5zkFopeB99"},"outputs":[],"source":["model = model1.to(device)\n","\n","predictions1 = []\n","for batch in sub_dataloader:\n","    # Add batch to GPU\n","    batch = tuple(t.to(device) for t in batch)\n","    # Unpack the inputs from our dataloader\n","    b_input_ids, b_input_mask = batch\n","    # Telling the model not to compute or store gradients,\n","    # saving memory and speeding up validation\n","    with torch.no_grad():\n","        # Forward pass, calculate validation loss\n","        logits = model(b_input_ids, b_input_mask)\n","        prediction1 = logits.max(1, keepdim=True)[1]\n","        predictions1.append(prediction1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ifFziRTaeVWc"},"outputs":[],"source":["model = model2.to(device)\n","\n","predictions2 = []\n","for batch in sub_dataloader:\n","    # Add batch to GPU\n","    batch = tuple(t.to(device) for t in batch)\n","    # Unpack the inputs from our dataloader\n","    b_input_ids, b_input_mask = batch\n","    # Telling the model not to compute or store gradients,\n","    # saving memory and speeding up validation\n","    with torch.no_grad():\n","        # Forward pass, calculate validation loss\n","        logits = model(b_input_ids, b_input_mask)\n","        prediction2 = logits.max(1, keepdim=True)[1]\n","        predictions2.append(prediction2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ubiNmmbMee88"},"outputs":[],"source":["model = model3.to(device)\n","\n","predictions3 = []\n","for batch in sub_dataloader:\n","    # Add batch to GPU\n","    batch = tuple(t.to(device) for t in batch)\n","    # Unpack the inputs from our dataloader\n","    b_input_ids, b_input_mask = batch\n","    # Telling the model not to compute or store gradients,\n","    # saving memory and speeding up validation\n","    with torch.no_grad():\n","        # Forward pass, calculate validation loss\n","        logits = model(b_input_ids, b_input_mask)\n","        prediction3 = logits.max(1, keepdim=True)[1]\n","        predictions3.append(prediction3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LLDpU1bce0QL"},"outputs":[],"source":["with open('/content/drive/MyDrive/공모전/submissions/pred1_kobart.pkl', 'wb') as f:\n","    pickle.dump(predictions1, f)\n","with open('/content/drive/MyDrive/공모전/submissions/pred2_kobart.pkl', 'wb') as f:\n","    pickle.dump(predictions2, f)\n","with open('/content/drive/MyDrive/공모전/submissions/pred3_kobart.pkl', 'wb') as f:\n","    pickle.dump(predictions3, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1221,"status":"ok","timestamp":1648520483442,"user":{"displayName":"권유진","userId":"17694849594474805342"},"user_tz":-540},"id":"sD4Kt0BLdDb5"},"outputs":[],"source":["with open('/content/drive/MyDrive/공모전/submissions/pred1_kobart.pkl', 'rb') as f:\n","    predictions1 = pickle.load(f)\n","with open('/content/drive/MyDrive/공모전/submissions/pred2_kobart.pkl', 'rb') as f:\n","    predictions2 = pickle.load(f)\n","with open('/content/drive/MyDrive/공모전/submissions/pred3_kobart.pkl', 'rb') as f:\n","    predictions3 = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1648520483443,"user":{"displayName":"권유진","userId":"17694849594474805342"},"user_tz":-540},"id":"Dx9NOS55dPZ2"},"outputs":[],"source":["predictions1 = torch.cat(predictions1).squeeze().tolist()\n","predictions2 = torch.cat(predictions2).squeeze().tolist()\n","predictions3 = torch.cat(predictions3).squeeze().tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":459,"status":"ok","timestamp":1648520522971,"user":{"displayName":"권유진","userId":"17694849594474805342"},"user_tz":-540},"id":"29ZVysa0d3I8"},"outputs":[],"source":["predictions1 = [idx2label_digit1[pred] for pred in predictions1]\n","predictions2 = [idx2label_digit2[pred] for pred in predictions2]\n","predictions3 = [idx2label_digit3[pred] for pred in predictions3]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":735,"status":"ok","timestamp":1648520541870,"user":{"displayName":"권유진","userId":"17694849594474805342"},"user_tz":-540},"id":"m7MqgVOQdw9H"},"outputs":[],"source":["sub = file1.copy()\n","sub['digit_1'] = predictions1\n","sub['digit_2'] = predictions2\n","sub['digit_3'] = predictions3"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1648520606272,"user":{"displayName":"권유진","userId":"17694849594474805342"},"user_tz":-540},"id":"_H2stYxFe6Ki"},"outputs":[],"source":["sub.to_csv('/content/drive/MyDrive/공모전/submissions/sub_kobart_220329.csv', index=False)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNsHEeaqtTdhXn2kJlKWYP0","machine_shape":"hm","mount_file_id":"15mAwufgFcixu1dfyzX0knbJHSEJuLNSF","name":"KoBART.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
