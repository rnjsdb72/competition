{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "420a40fe",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1589657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from selenium.webdriver import Chrome\n",
    "import requests\n",
    "import lxml.html\n",
    "import os\n",
    "from glob import glob\n",
    "import olefile\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from kiwipiepy import Kiwi\n",
    "from wordcloud import WordCloud\n",
    "import time\n",
    "from itertools import combinations\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "plt.rc('font',family='malgun gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcca8f7",
   "metadata": {},
   "source": [
    "### Web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd9c3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scrapping with selenium\n",
    "texts=[]\n",
    "browser = Chrome()\n",
    "for p in tqdm(range(1,118)): # 페이지 이동\n",
    "    try: \n",
    "        browser.get(f'https://www.kmst.go.kr/kmst/verdict/writtenVerdict/selectWrittenVerdict.do?pageIndex={p}')\n",
    "        urls = browser.find_elements_by_css_selector('dt a')\n",
    "        for i in range(len(urls)-1): # 첫 게시글부터 마지막 게시글까지 이동\n",
    "                urls[i].click()\n",
    "                if browser.find_elements_by_css_selector('span.btn_pack.c_blue') != []: # 서버에서 게시글 들어가지 못하게 할 때\n",
    "                    browser.find_elements_by_css_selector('span.btn_pack.c_blue')[0].click() # 돌아가기 버튼 클릭\n",
    "                    urls[i].click() # 다시 한 번 더 게시글 진입 시도\n",
    "                text = browser.find_elements_by_css_selector('dd.description')[0].text # 게시글 내용 선택\n",
    "                texts.append(text) # 스크래핑\n",
    "                browser.back() # 뒤로 가기\n",
    "                urls = browser.find_elements_by_css_selector('dt a')\n",
    "    except: # 오류 발생 시\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a578bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = pd.DataFrame({'text':texts}).reset_index() # 스크래핑한 내용 데이터프레임화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2579d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents.text = documents.text.map(lambda x: re.sub(r'\\n+','\\n',x)) # 엔터 여러 번 했을 경우 한 번으로 통일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc0996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to separate morpheme & extract noun\n",
    "def extract_n(text):\n",
    "    kiwi = Kiwi()\n",
    "    kiwi.prepare()\n",
    "    morpheme = kiwi.analyze(text) # 형태소 분석\n",
    "    for lemma, pos, _, _ in morpheme[0][0]: # 표제어, 품사 추출\n",
    "        if pos.startswith('N'): # 품사가 명사일 경우\n",
    "            yield lemma # 표제어 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011a0e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacing_non(word,text): # 게시글별로 키워드 입력 방식 달라 통일(띄어쓰기)\n",
    "    spacing_list = []\n",
    "    for i in range(len(word)):\n",
    "        raw_x = word # 단어 raw 형태\n",
    "        for split in combinations(word,i): # i개로 글자 나눔\n",
    "            x = raw_x\n",
    "            for s in split:\n",
    "                x = x.replace(s,s+' ') # 글자 나눈 사이에 공백 추가\n",
    "            spacing_list.append(x)\n",
    "    for f in spacing_list:\n",
    "        text =text.replace(f,raw_x) # 여러 경우의 수 하나로 통일\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82112adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_key(x):\n",
    "    try:\n",
    "        if x == '@DataType:Clob': # 이상한 형태일 경우 넘어간다.\n",
    "            return\n",
    "        x = re.sub(r'\\n+','\\n',x) # 엔터키 여러 번 실행했을 경우 한 번으로 통일\n",
    "        x = x.replace('이 유','').replace('1. 사 실','').replace('?','.') # 불필요한 요소들 대체\n",
    "        x = x.replace('5. 사고방지 교훈','4. 사고방지 교훈') # 오탈자 대체\n",
    "        for key in ['주문','선명','선적항','선박소유자','총톤수','기관종류.출력','해양사고관련자','직명','면허의종류','사고일시','사고장소','2.원인','가.원인고찰',\n",
    "         '나.사고발생원인','다.해양사고관련자의긴급피난주장에대한검토','3.해양사고관련자의행위','4.사고방지교훈']:\n",
    "            x = spacing_non(key,x) # 키워드 형태 통일\n",
    "        x1 = pd.Series(x.splitlines())\n",
    "        if '따라서 주문과 같이 재결한다.' in x: # 내용 중 필요한 부분만 발췌\n",
    "            content = x1.iloc[x1[x1.map(lambda x: x.endswith('호') and x.startswith('해심',2,4))].index[0]\n",
    "                          :x1[x1.map(lambda x: x=='따라서 주문과 같이 재결한다.')].index[0]].reset_index(drop=True)\n",
    "        else:\n",
    "            try:\n",
    "                content = x1.iloc[x1[x1.map(lambda x: x.endswith('호') and x.startswith('해심',2,4))].index[0]:].reset_index(drop=True)\n",
    "            except:\n",
    "                content = x1\n",
    "        if x1[x1.map(lambda x: x.endswith('호') and x.startswith('해심',2,4))].reset_index(drop=True)[0][:2] == '부산': # 부산일 경우\n",
    "            key_idx_list = []\n",
    "            keys1 = ['주문','선명','선적항','선박소유자','총톤수','기관종류.출력','해양사고관련자','직명','면허의종류','사고일시','사고장소']\n",
    "            keys2 = ['2.원인','가.원인고찰', '나.사고발생원인','다.해양사고관련자의긴급피난주장에대한검토','3.해양사고관련자의행위','4.사고방지교훈']\n",
    "            keys = keys1+keys2\n",
    "            for a in keys1: # 키워드 인덱스 번호 가져오기\n",
    "                if list(content[content == a]) == []:\n",
    "                    key_idx_list.append(None)\n",
    "                else:\n",
    "                    key_idx_list.append(content[content == a].index[-1])\n",
    "            key_idx_list.append(key_idx_list[-1]+2)\n",
    "            key_con=[]\n",
    "            for a in keys2:\n",
    "                if list(content[content == a]) == []:\n",
    "                    key_idx_list.append(None)\n",
    "                else:\n",
    "                    key_idx_list.append(content[content == a].index[-1])\n",
    "            for i in range(len(key_idx_list)-1): # 키워드 없을 경우 결측값 대입\n",
    "                if key_idx_list[i] == None:\n",
    "                    key_con.append(None)\n",
    "                elif key_idx_list[i+1] == None:\n",
    "                    if len(key_idx_list) <= i+2:\n",
    "                        key_con.append(' '.join(list(content[key_idx_list[i]+1:])))\n",
    "                    else:\n",
    "                        key_con.append(' '.join(list(content[key_idx_list[i]+1:key_idx_list[i+2]])))\n",
    "                else: # 문장 합치기\n",
    "                    key_con.append(' '.join(list(content[key_idx_list[i]+1:key_idx_list[i+1]])))\n",
    "            if key_idx_list[-1] != None:\n",
    "                key_con.append(' '.join(list(content[key_idx_list[-1]+1:])))\n",
    "            else:\n",
    "                key_con.append(None)\n",
    "            df = pd.DataFrame(np.array([[x1.iloc[x1[x1.map(lambda x: x.endswith('호') and x.startswith('해심',2,4))].index[0]+1]]+key_con])\n",
    "                                      ,columns=(['사건명']+keys1+['사건상세']+keys2)) # 데이터프레임으로 만들기\n",
    "            return df # 이하동문\n",
    "        elif x1[x1.map(lambda x: x.endswith('호') and x.startswith('해심',2,4))].reset_index(drop=True)[0][:2] == '인천':\n",
    "            for word in ['주문','선명','선적항','선박소유자','총톤수','기관종류출력','해양사고관련자','직명','면허의종류','사고일시','사고장소']:\n",
    "                x = x.replace(word,f'{word}'+'\\n')\n",
    "            x = x.replace('3. 해양사고관련자\\n의 행위','3. 해양사고관련자의 행위')\n",
    "            x1 = pd.Series(x.splitlines())\n",
    "            content = x1.iloc[x1[x1.map(lambda x: x.endswith('호') and x.startswith('해심',2,4))].index[0]:].reset_index(drop=True)\n",
    "            key_idx_list = []\n",
    "            keys1 = ['주문','선명','선적항','선박소유자','총톤수','기관종류ㆍ출력','해양사고관련자','직명','면허의종류','사고일시','사고장소']\n",
    "            keys2 = ['2.원인','가.원인고찰', '나.사고발생원인','다.해양사고관련자의긴급피난주장에대한검토','3.해양사고관련자의행위','4.사고방지교훈']\n",
    "            keys = keys1+keys2\n",
    "            for a in keys1:\n",
    "                if list(content[content == a]) == []:\n",
    "                    key_idx_list.append(None)\n",
    "                else:\n",
    "                    key_idx_list.append(content[content == a].index[-1])\n",
    "            key_idx_list.append(key_idx_list[-1]+2)\n",
    "            key_con=[]\n",
    "            for a in keys2:\n",
    "                if list(content[content == a]) == []:\n",
    "                    key_idx_list.append(None)\n",
    "                else:\n",
    "                    key_idx_list.append(content[content == a].index[-1])\n",
    "            for i in range(len(key_idx_list)-1):\n",
    "                if key_idx_list[i] == None:\n",
    "                    key_con.append(None)\n",
    "                elif key_idx_list[i+1] == None:\n",
    "                    if len(key_idx_list) <= i+2:\n",
    "                        key_con.append(' '.join(list(content[key_idx_list[i]+1:])))\n",
    "                    else:\n",
    "                        key_con.append(' '.join(list(content[key_idx_list[i]+1:key_idx_list[i+2]])))\n",
    "                else:\n",
    "                    key_con.append(' '.join(list(content[key_idx_list[i]+1:key_idx_list[i+1]])))\n",
    "            if key_idx_list[-1] != None:\n",
    "                key_con.append(' '.join(list(content[key_idx_list[-1]+1:])))\n",
    "            else:\n",
    "                key_con.append(None)\n",
    "            df = pd.DataFrame(np.array([[x1.iloc[x1[x1.map(lambda x: x.endswith('호') and x.startswith('해심',2,4))].index[0]+1]]+key_con])\n",
    "                                      ,columns=(['사건명']+keys1+['사건상세']+keys2)).rename(columns={'기관종류ㆍ출력':'기관종류.출력'})\n",
    "            return df\n",
    "        elif x1[x1.map(lambda x: x.endswith('호') and x.startswith('해심',2,4))].reset_index(drop=True)[0][:2] == '동해':\n",
    "            x = x.replace('사고 장소','사고장소')\n",
    "            for word in ['주문','선명','선적항','선박소유자','총톤수','기관종류.출력','해양사고관련자','직명','면허의종류','사고일시','사고장소']:\n",
    "                x = x.replace(word,f'{word}'+'\\n')\n",
    "            x = x.replace('3. 해양사고관련자\\n의 행위','3. 해양사고관련자의 행위')\n",
    "            x1 = pd.Series(x.splitlines())\n",
    "            content = x1.iloc[x1[x1.map(lambda x: x.endswith('호') and x.startswith('해심',2,4))].index[0]:].reset_index(drop=True)\n",
    "            key_idx_list = []\n",
    "            keys1 = ['주문','선명','선적항','선박소유자','총톤수','기관종류.출력','해양사고관련자','직명','면허의종류','사고일시','사고장소']\n",
    "            keys2 = ['2.원인','가.원인고찰', '나.사고발생원인','다.해양사고관련자의긴급피난주장에대한검토','3.해양사고관련자의행위','4.사고방지교훈']\n",
    "            keys = keys1+keys2\n",
    "            for a in keys1:\n",
    "                if list(content[content == a]) == []:\n",
    "                    key_idx_list.append(None)\n",
    "                else:\n",
    "                    key_idx_list.append(content[content == a].index[-1])\n",
    "            key_idx_list.append(key_idx_list[-1]+2)\n",
    "            key_con=[]\n",
    "            for a in keys2:\n",
    "                if list(content[content == a]) == []:\n",
    "                    key_idx_list.append(None)\n",
    "                else:\n",
    "                    key_idx_list.append(content[content == a].index[-1])\n",
    "            for i in range(len(key_idx_list)-1):\n",
    "                if key_idx_list[i] == None:\n",
    "                    key_con.append(None)\n",
    "                elif key_idx_list[i+1] == None:\n",
    "                    if len(key_idx_list) <= i+2:\n",
    "                        key_con.append(' '.join(list(content[key_idx_list[i]+1:])))\n",
    "                    else:\n",
    "                        key_con.append(' '.join(list(content[key_idx_list[i]+1:key_idx_list[i+2]])))\n",
    "                else:\n",
    "                    key_con.append(' '.join(list(content[key_idx_list[i]+1:key_idx_list[i+1]])))\n",
    "            if key_idx_list[-1] != None:\n",
    "                key_con.append(' '.join(list(content[key_idx_list[-1]+1:])))\n",
    "            else:\n",
    "                key_con.append(None)\n",
    "            df = pd.DataFrame(np.array([[x1.iloc[x1[x1.map(lambda x: x.endswith('호') and x.startswith('해심',2,4))].index[0]+1]]+key_con])\n",
    "                                      ,columns=(['사건명']+keys1+['사건상세']+keys2))\n",
    "            return df\n",
    "        elif x1[x1.map(lambda x: x.endswith('호') and x.startswith('해심',2,4))].reset_index(drop=True)[0][:2] == '목포':\n",
    "            key_idx_list = []\n",
    "            keys1 = ['주문','선명','선적항','선박소유자','총톤수','기관종류.출력','해양사고관련자','직명','면허의종류','사고일시','사고장소']\n",
    "            keys2 = ['2.원인','가.원인고찰', '나.사고발생원인','다.해양사고관련자의긴급피난주장에대한검토','3.해양사고관련자의행위','4.사고방지교훈']\n",
    "            keys = keys1+keys2\n",
    "            for a in keys1:\n",
    "                if list(content[content == a]) == []:\n",
    "                    key_idx_list.append(None)\n",
    "                else:\n",
    "                    key_idx_list.append(content[content == a].index[-1])\n",
    "            key_idx_list.append(key_idx_list[-1]+2)\n",
    "            key_con=[]\n",
    "            for a in keys2:\n",
    "                if list(content[content == a]) == []:\n",
    "                    key_idx_list.append(None)\n",
    "                else:\n",
    "                    key_idx_list.append(content[content == a].index[-1])\n",
    "            for i in range(len(key_idx_list)-1):\n",
    "                if key_idx_list[i] == None:\n",
    "                    key_con.append(None)\n",
    "                elif key_idx_list[i+1] == None:\n",
    "                    if len(key_idx_list) <= i+2:\n",
    "                        key_con.append(' '.join(list(content[key_idx_list[i]+1:])))\n",
    "                    else:\n",
    "                        key_con.append(' '.join(list(content[key_idx_list[i]+1:key_idx_list[i+2]])))\n",
    "                else:\n",
    "                    key_con.append(' '.join(list(content[key_idx_list[i]+1:key_idx_list[i+1]])))\n",
    "            if key_idx_list[-1] != None:\n",
    "                key_con.append(' '.join(list(content[key_idx_list[-1]+1:])))\n",
    "            else:\n",
    "                key_con.append(None)\n",
    "            df = pd.DataFrame(np.array([[x1.iloc[x1[x1.map(lambda x: x.endswith('호') and x.startswith('해심',2,4))].index[0]+1]]+key_con])\n",
    "                                      ,columns=(['사건명']+keys1+['사건상세']+keys2))\n",
    "            return df\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02bdbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_keys = documents.text.map(split_key) # 재결서 데이터프레임화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ce1d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "written_judges = pd.concat(list(documents_keys),axis=0).reset_index(drop=True) # 재결서 데이터프레임 하나로 모두 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c10ff4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 결측값 처리 (키워드에 잘 못 들어간 요소들 처리)\n",
    "written_judges.loc[written_judges['선박소유자'].map(lambda x: '톤 수' in x),'총톤수'] = written_judges[written_judges['선박소유자'].map(lambda x: '톤 수' in x)]['선박소유자'].map(lambda x: x.split('톤 수')).map(lambda x:x[1])\n",
    "written_judges.loc[written_judges['선박소유자'].map(lambda x: '톤 수' in x),'선박소유자'] = written_judges[written_judges['선박소유자'].map(lambda x: '톤 수' in x)]['선박소유자'].map(lambda x: x.split('톤 수')).map(lambda x:x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620ba674",
   "metadata": {},
   "outputs": [],
   "source": [
    "written_judges.loc[written_judges['선박소유자'].map(lambda x: '총톤수' in x),'총톤수'] = written_judges[written_judges['선박소유자'].map(lambda x: '총톤수' in x)]['선박소유자'].map(lambda x: x.split('총톤수')).map(lambda x:x[1])\n",
    "written_judges.loc[written_judges['선박소유자'].map(lambda x: '총톤수' in x),'선박소유자'] = written_judges[written_judges['선박소유자'].map(lambda x: '총톤수' in x)]['선박소유자'].map(lambda x: x.split('총톤수')).map(lambda x:x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e692afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "written_judges.loc[written_judges['총톤수'].map(lambda x: '기관종류' in x),'기관종류.출력'] = written_judges.loc[written_judges['총톤수'].map(lambda x: '기관종류' in x),'총톤수'].map(\n",
    "    lambda x: x.split('기관종류')[1]).map(lambda x: x.replace('ㆍ출력','').replace('․출력',''))\n",
    "written_judges.loc[written_judges['총톤수'].map(lambda x: '기관종류' in x),'총톤수'] = written_judges.loc[written_judges['총톤수'].map(lambda x: '기관종류' in x),'총톤수'].map(lambda x: x.split('기관종류')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5af457",
   "metadata": {},
   "outputs": [],
   "source": [
    "written_judges.to_csv('written_judges.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cdd911",
   "metadata": {},
   "source": [
    "### 사건 상세"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40317f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사건 상세 행 단어문서행렬로 만들기, 빈도 나타내기\n",
    "cv2 = CountVectorizer(tokenizer=extract_n)\n",
    "tdm2 = cv2.fit_transform(written_judges['사건상세'])\n",
    "df2 = pd.DataFrame({'단어':cv2.get_feature_names(), '빈도':tdm2.sum(axis=0).flat})\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7be5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "df2.to_csv('wordcounts2.csv')\n",
    "np.save('tdm2.npy', tdm2)\n",
    "with open('cv2.pkl','wb') as f:\n",
    "    pickle.dump(cv2,f)\n",
    "# 불러오기\n",
    "with open('cv2.pkl', 'rb') as f:\n",
    "    cv2 = pickle.load(f)\n",
    "tdm2 = np.load('tdm2.npy', allow_pickle=True).tolist()\n",
    "df2 = pd.read_csv('wordcounts2.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620cf2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lists = []\n",
    "for text in tqdm(written_judges['사건상세'].dropna()): # 내용 형태소 분석\n",
    "    n_list = []\n",
    "    kiwi=Kiwi()\n",
    "    kiwi.prepare()\n",
    "    morpheme = kiwi.analyze(text)\n",
    "    for lemma, pos, _, _ in morpheme[0][0]:\n",
    "        if pos.startswith('N'): # 명사만 추출\n",
    "            n_list.append(lemma)\n",
    "    n_lists.append(n_list)\n",
    "\n",
    "n_lists = pd.Series(n_lists) # 추출한 명사 합치기\n",
    "\n",
    "ft2 = FastText(sg=1,sentences=n_lists) # FastText기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bb581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents = ['기관손상', '안전사고', '좌초', '부유물감김', '해양오염', '충돌', '전복', '화재 · 폭발', '침몰', '운항저해', '접촉']\n",
    "similarities = []\n",
    "for acc in combinations(accidents,2): # 사고 간 코사인 유사도 검사\n",
    "    similarities.append((acc, ft2.wv.similarity(*acc)))\n",
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6245fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df = pd.DataFrame(similarities, columns=['acc','sim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6481647",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df.to_csv('sim_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5b32ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_sm = sim_df.set_index('acc').sort_values('sim',ascending=False) # 코사인 유사도를 기준으로 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0c2929",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_sm.plot.bar(figsize=(12,4), legend=False)\n",
    "plt.title('사고 유형별 코사인 유사도')\n",
    "plt.ylabel('cosine similarity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2079a406",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_lists = []\n",
    "for i in accidents: # 단어 별 코사인 유사도 행렬 만들기\n",
    "    sm_list = []\n",
    "    for j in accidents:\n",
    "        sm_list.append(ft2.wv.similarity(i,j))\n",
    "    sm_lists.append(sm_list)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(pd.DataFrame(columns=accidents,index=accidents,data=sm_lists), annot=True) # 히트맵 그리기\n",
    "plt.title('사고 간 코사인 유사도')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b85080",
   "metadata": {},
   "source": [
    "### 사고발생원인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c46cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사고발생원인 행 단어문서행렬로 만들기, 빈도분석\n",
    "cv3 = CountVectorizer(tokenizer=extract_n)\n",
    "tdm3 = cv3.fit_transform(written_judges['나.사고발생원인'].dropna())\n",
    "df3 = pd.DataFrame({'단어':cv3.get_feature_names(), '빈도':tdm3.sum(axis=0).flat})\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c40ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "df3.to_csv('wordcounts3.csv')\n",
    "np.save('tdm3.npy', tdm3)\n",
    "with open('cv3.pkl','wb') as f:\n",
    "    pickle.dump(cv3,f)\n",
    "# 불러오기\n",
    "with open('cv3.pkl', 'rb') as f:\n",
    "    cv3 = pickle.load(f)\n",
    "tdm3 = np.load('tdm3.npy', allow_pickle=True).tolist()\n",
    "df3 = pd.read_csv('wordcounts3.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64da82ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lists = []\n",
    "for text in tqdm(written_judges['나.사고발생원인'].dropna()): # 내용 형태소 분석\n",
    "    n_list = []\n",
    "    kiwi=Kiwi()\n",
    "    kiwi.prepare()\n",
    "    morpheme = kiwi.analyze(text)\n",
    "    for lemma, pos, _, _ in morpheme[0][0]:\n",
    "        if pos=='NNG': # 일반 명사만 추출\n",
    "            n_list.append(lemma)\n",
    "    n_lists.append(n_list)\n",
    "\n",
    "n_lists = pd.Series(n_lists)\n",
    "\n",
    "ft3 = FastText(sg=1,sentences=n_lists) # FastText기법 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eae801",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_acc = []\n",
    "for acc in accidents:\n",
    "    ms_acc.append([acc,ft3.wv.most_similar(acc, topn=100)]) # 사고 별로 가장 많이 나오는 단어 100개 나타내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f622f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_df = pd.DataFrame(ms_acc, columns=['acc','ms_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c0d4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_df['ms_list'] = ms_df.ms_list.map(lambda x: pd.DataFrame(x, columns=['word','sm']).set_index('word'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe4e8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# barchart로 나타내기\n",
    "fig, axes = plt.subplots(11,1,figsize=(20,44))\n",
    "for i in tqdm(range(ms_df.shape[0])):\n",
    "    ms_df.ms_list[i].plot.bar(ax=axes[i],legend=False, xlabel=False, color='gray')\n",
    "    axes[i].set_title(f'{ms_df.acc[i]}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9beb65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 클러스터링\n",
    "cl = DBSCAN(min_samples=2)\n",
    "result_cl = pd.DataFrame(tuple(zip(ft2.wv.index_to_key,cl.fit_predict(ft2.wv.vectors))), columns=['word','cl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd81574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(11):\n",
    "    ms_df.ms_list[i] = pd.merge(ms_df.ms_list[i].reset_index(),result_cl,on='word',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95782c36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cl_count = []\n",
    "for i in range(11):\n",
    "    cl_count.append([ms_df.acc[i],ms_df.ms_list[i].cl.value_counts().shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a72f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cl_count,columns=['acc','num_reason']).set_index('acc').plot.bar(xlabel=False,legend='', color='coral')\n",
    "plt.title('사고 종류별 요인 개수')\n",
    "plt.ylabel('요인 개수')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61442d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_df.to_csv('ms_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
